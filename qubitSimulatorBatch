#!/bin/sh
#  gofluent <Number of Nodes>x<Number of Processors> <JournalFile.jou> <DAYS-HOURS:MINS>
# run the default time
#  gofluent <Number of Nodes>x<Number of Processors> <JournalFile.jou>  
# runs the default number of processors and time
#  gofluent <JournalFile.jou>

cpus=36
nodes=1
name=$1
simDirectory=$2
maxRunTime=$3

fileName="/quantizeQubitSimulator.py"
qubitSimulatorFile=$simDirectory$fileName

export NAME=$name
TIME="00:$maxRunTime:00" #Days-Hours:Mins for running time 
if [ $maxRunTime -lt 10 ];
then
	TIME="00:0${maxRunTime}:00"
fi;
#echo $TIME

cd $simDirectory

echo Submitting job to SLURM

#Below is the script that is submited to SLURM by the command "sbatch"
#To check on the job run "squeue" or "scancel JOBID" to stop, where JOBID is number found in squeue.

echo "#!/bin/sh 
#SBATCH --mem=381296 # MB
#SBATCH --time=$TIME
#SBATCH --nodes=$nodes
#SBATCH -n $cpus
#SBATCH -A 2006240353
#SBATCH --export=ALL
## -p is the partition or group name of the nodes to run on "compute" is the common nodes
#SBATCH -p compute
## You can run --overcommit to have the other CPUs on the node available to users
## or you can run --exclusive allowing only on job.
#SBATCH --exclusive
#SBATCH --mail-type=ALL
#SBATCH --mail-user=$USER@mymail.mines.edu

# See slurm.JOBID.out for results of these commands
echo \$SLURM_JOBID
export SLURM_JOB_NAME=$NAME.\$SLURM_JOBID
echo Starting SLURM job \$SLURM_JOB_NAME
echo Number of NODES:\$SLURM_NNODES
echo Number of Processors:\$SLURM_NPROCS

cd \$SLURM_SUBMIT_DIR
# makes a unique directory to store output files in
# If re-runing script comment out these 4 lines and run "sbatch script.JOBID" to try again
#mkdir \$SLURM_JOB_NAME
#cd \$SLURM_JOB_NAME
#mv \$SLURM_SUBMIT_DIR/\$ANSYSSIMULATORFILE ./\$SLURM_JOB_NAME.py
#cp \$SLURM_SUBMIT_DIR/*.c \$SLURM_SUBMIT_DIR/\$SLURM_JOB_NAME/.
#ln -s ../*.cas.gz ./.


# Outputs this script to a file
cat \$0 > script.\$SLURM_JOBID

#Counts the number of hosts and calculates the total number of CPUs to use
export ncpus=\`/sw/utility/local/expands \$SLURM_JOB_NODELIST | wc -l\`
echo nodelist found this many CPUs:\$ncpus

# create a list of nodes in the file nodes. First listed is the Fluent host node
/sw/utility/local/expands \$SLURM_JOB_NODELIST > nodes

# added -slurm for job scheduler
# add -mpitest for network connect test and will NOT run .jou file

python $qubitSimulatorFile
" > $NAME.runjob

sbatch $NAME.runjob
#rm $NAME.runjob

